import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from docx import Document
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import FAISS
from ChatGLM3_main.test import main as chat
from rank_bm25 import BM25Okapi 
import jieba
doc_path = "D:/Age_copy/law_doc"

documnets = []
def load_docx(file_path):
    doc = Document(file_path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return text

for file_name in os.listdir(doc_path):
    if file_name.endswith(".docx"):
        file_path = os.path.join(doc_path,file_name)
        try:
            text = load_docx(file_path)
            documnets.append(text)
        except Exception as e:
            print(f"è¯»å–{file_name}å¤±è´¥:{e}")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=128,chunk_overlap=50)
docs = text_splitter.create_documents(documnets)
print(f"æˆåŠŸè½½å…¥{len(docs)}ä¸ªæ–‡æ¡£å—")

# Step 3 Convert Documents inot Embeddings
embdding_model = HuggingFaceBgeEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs,embdding_model)
vector_store.save_local("faiss_index")
print("æ–‡æ¡£å·²å­˜å…¥FAISSå‘é‡æ•°æ®åº“")

# Step4 Build BM25 index
bm25_corpus = [list(jieba.cut(doc.page_content)) for doc in docs]
bm25 = BM25Okapi(bm25_corpus)

# Step5 RAG combine BM25 with Embedding Similiarity Search
def rag_ask(query):
     max_lenght=100
     # BM25 keyword search
     tokenized_query = list(jieba.cut(query))
     bm25_scores = bm25.get_scores(tokenized_query)

     #  Get BM25 Top 5 documents
     bm25_top_k = 3
     bm25_top_docs = sorted(zip(bm25_scores,docs),key=lambda x:x[0],reverse=True)[:bm25_top_k]

     # From BM25 outcome select top 3 docs for Embedding Search
     retrieved_docs = [doc[1] for doc in bm25_top_docs]
     embedding_retrieved_docs = vector_store.similarity_search(query,k=2)  # vetor search

     # Combine BM25 and Embedding Search Results
     combined_docs = {doc.page_content:doc for doc in retrieved_docs + embedding_retrieved_docs}
     retrieved_docs = list(combined_docs.values())[:2]

     context = "\n\n".join([doc.page_content for doc in retrieved_docs])
     print(context)
     prompt = f"ä»¥ä¸‹æ˜¯ç›¸å…³èµ„æ–™ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜:{query}\n\nè¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"
     response = chat(prompt,1)
     return response

query = "å’³å—½å¸¦ç—°ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ"
response = rag_ask(query)
print(f"ğŸ’¬ LLM å›ç­”:\n{response}")